{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T15:52:25.749970Z",
     "start_time": "2025-11-11T15:52:25.622476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = {}\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    #初始化权重，seed作为种子控制权重每次生成一致，保证相同数据下训练得出的结论相同\n",
    "    #每一层的权重都会受到上一层的神经元（layer_size）的影响，根据上一层神经元的大小控制本层权重的大小\n",
    "    #其中计算权重的方法能够实现“均值 0、方差合适”的权重初始化（*后面的数据是缩放因子->方差调节器，每个激化函数都有对应的调节器）\n",
    "    #若方差较大（权重计算方法不对），则对一层一层传递下去，导致输出的隐藏值过大，损失值爆炸，因此缩放因子不能随意更改\n",
    "    #权重有正有负 —— 正权重代表 “增强输入信号”，负权重代表 “削弱输入信号\n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(42)\n",
    "        for l in range(1, len(self.layer_sizes)):\n",
    "            self.parameters[f\"W{l}\"] = np.random.randn(\n",
    "                self.layer_sizes[l], self.layer_sizes[l - 1]) * np.sqrt(2.0 / self.layer_sizes[l - 1])\n",
    "            self.parameters[f\"b{l}\"] = np.zeros(self.layer_sizes[l])\n",
    "\n",
    "    #激化函数，该函数是完美适配的，能够将产生的负数据全部移除，避免后续产生梯度消失的情况\n",
    "    #负数据的产生：权重 W 有负数，线性计算Z=W×A_prev + b中 “削弱作用超过增强作用” 的自然结果；\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    #梯度开关，关闭负数据的梯度，避免产生梯度消失问题\n",
    "    def relu_derivative(self, Z):\n",
    "        #反向传播的 “梯度开关”(0->梯度废弃，1->梯度下传)\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    #将获取的隐藏层数据Z转化成概率分布数据（所有隐藏层获取的数据为1，相当于对真实值每个位置的预测数据）\n",
    "    def softmax(self, Z):\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        #将我们预测的样本总数据变成总和为1的概率分布，用每个样本的数据除以样本总数据和\n",
    "        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "\n",
    "    #前向传播获取到预测的输出数据，并将隐藏层数据进行激化，输出层数据进行softmax，保存在缓存中用于后续反向传播\n",
    "    #前向传播公式：Z=W*A(l-1)+b\n",
    "    def forward_propagation(self, X):\n",
    "        cache = {\"A0\": X}\n",
    "        L = len(self.parameters) / 2\n",
    "        for l in range(1, L):\n",
    "            cache[f\"Z{l}\"] = np.dot(self.parameters[f\"W{l}\"], cache[f\"A{l - 1}\"]) + self.parameters[f\"b{l}\"]\n",
    "            Z = self.relu(cache[f\"Z{l}\"])\n",
    "\n",
    "        #Z是代表的被激活后的数据，A则是代表的原始权重、偏置后的数据以及输入和最后softmax处理后的输出层数据\n",
    "        cache[f\"Z{L}\"] = np.dot(self.parameters[f\"W{L}\"], self.parameters[f\"A{L - 1}\"]) + self.parameters[f\"b{L}\"]\n",
    "        cache[f\"A{L - 1}\"] = self.softmax(cache[f\"Z{L}\"])\n",
    "\n",
    "        return cache\n",
    "\n",
    "\n",
    "    def compute_loss(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        #因为Y是one-hot值，所以只有真实数据所在位置为1，其它位置都为零，这样就可以算出AL对应真实数据的概率，把所有概率相加（因为Log（AL）为负数，所有前面加-改为正数直观看出和真实数据的差距）\n",
    "        loss = -np.sum(Y * np.log(AL + 1e-8)) / m\n",
    "        return loss\n",
    "\n",
    "    #输出层是 “概率输出 + 交叉熵损失” 的组合，梯度有专属简化公式；隐藏层是 “ReLU 激活”，梯度需要 “接收下一层梯度 + 筛选”，两者逻辑不同，必须分开处理。\n",
    "    #反向传播：通过预测的输出数据一层一层向上推导，计算每一层权重的导数（导数越大说明对当前数据影响越偏离，对最终数据影响越大）\n",
    "    def backward_propagation(self, X, Y, cache):\n",
    "        m=X.shape[1]\n",
    "        L=len(self.layer_sizes) // 2\n",
    "        grads={}\n",
    "\n",
    "        #Softmax + 交叉熵求导简化公式\n",
    "        dZ=cache[f\"A{L}\"]-Y\n",
    "        #把包装车间的误差，通过 “加工方式 W3” 倒推到第二道车间的输出 A2\n",
    "        grads[f\"dW{L}\"]=np.dot(dZ,cache[f\"A{L - 1}\"].T)/m\n",
    "\n",
    "        for l in range(1, L):\n",
    "            dA=np.dot(self.parameters[f\"W{l+1}\"], dZ)\n",
    "            dZ=dA*self.relu_derivative(cache[f\"Z{l}\"])\n",
    "            grads[f\"dW{l}\"]=np.dot(dZ, cache[f\"A{l-1}\"].T)/m\n",
    "            grads[f\"db{l}\"]=np.sum(dZ, axis=1, keepdims=True)/m\n",
    "\n",
    "        return grads\n",
    "\n",
    "    #根据反向传播计算出的每一层权重偏导数重新更新当前权重，通过学习率调节更新力度\n",
    "    def update_paramaters(self,grads):\n",
    "        L=len(self.layer_sizes) // 2\n",
    "        for l in range(1, L):\n",
    "            self.parameters[f\"W{l}\"]-= self.learning_rate * grads[f\"dW{l}\"]\n",
    "            self.parameters[f\"b{l}\"]-= self.learning_rate * grads[f\"db{l}\"]\n",
    "\n",
    "\n",
    "    def prdict(self,X):\n",
    "        cache = self.forward_propagation(X)\n",
    "        L=len(self.layer_sizes) // 2\n",
    "        predictions=np.argmax(cache[f\"A{L}\"], axis=0)\n",
    "        return predictions\n",
    "\n",
    "    #argmax函数相当于将预测数据转化为one-hot样式，6000个样本中每个预测的数据位置，和样本中真实值的位置数据比较（true->1,false->0,再通过np.mean函数求取true和false所占的比例）\n",
    "    def accuracy(self, X, Y):\n",
    "        predictions=self.prdict(X)\n",
    "        true_labels=np.argmax(Y, axis=0)\n",
    "        accuracy=np.mean(predictions==true_labels)\n",
    "        return accuracy\n",
    "\n",
    "    #\n",
    "\n",
    "\n"
   ],
   "id": "19e7938ee1b8ca6a",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
